{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Laboratorium: Analiza obraz√≥w przy pomocy sieci konwolucyjnych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ≈Åadowanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Do za≈Çadowania danych skorzystamy z pakietu Tensorflow Datasets, kt√≥ry udostƒôpnia wiele zbior√≥w przydatnych do uczenia maszynowego. Aby utrzymaƒá wzglƒôdnie kr√≥tkie czasy uczenia, do ƒáwicze≈Ñ bƒôdziemy u≈ºywaƒá zbioru tf_flowers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "[test_set_raw, valid_set_raw, train_set_raw], info = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True,\n",
    "    with_info=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kilka s≈Ç√≥w o argumentach metody load:\n",
    "- split zapewnia odpowiedni podzia≈Ç zbioru (dlatego pierwszy element zwracanej krotki jest 3-elementowym s≈Çownikiem),\n",
    "- as_supervised sprawia, ≈ºe zwracane obiekty tf.data.Dataset majƒÖ postaƒá krotek zawierajƒÖcych zar√≥wno cechy, jak i etykiety,\n",
    "- with_info dodaje drugi element zwracanej krotki.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='tf_flowers',\n",
       "    full_name='tf_flowers/3.0.1',\n",
       "    description=\"\"\"\n",
       "    A large set of images of flowers\n",
       "    \"\"\",\n",
       "    homepage='https://www.tensorflow.org/tutorials/load_data/images',\n",
       "    data_path='~\\\\tensorflow_datasets\\\\tf_flowers\\\\3.0.1',\n",
       "    file_format=tfrecord,\n",
       "    download_size=218.21 MiB,\n",
       "    dataset_size=221.83 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=5),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'train': <SplitInfo num_examples=3670, num_shards=2>,\n",
       "    },\n",
       "    citation=\"\"\"@ONLINE {tfflowers,\n",
       "    author = \"The TensorFlow Team\",\n",
       "    title = \"Flowers\",\n",
       "    month = \"jan\",\n",
       "    year = \"2019\",\n",
       "    url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\" }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mo≈ºemy ≈Çatwo wyekstrahowaƒá istotne parametry zbioru:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_names = info.features[\"label\"].names\n",
    "n_classes = info.features[\"label\"].num_classes\n",
    "dataset_size = info.splits[\"train\"].num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wy≈õwietlmy kilka przyk≈Çadowych obraz√≥w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "index = 0\n",
    "sample_images = train_set_raw.take(9)\n",
    "for image, label in sample_images:\n",
    "    index += 1\n",
    "    plt.subplot(3, 3, index)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Class: {}\".format(class_names[label]))\n",
    "    plt.axis(\"off\")\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Budujemy prostƒÖ sieƒá CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym ƒáwiczeniu zbudujemy sieƒá o nieskompikowanej strukturze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  Przygotowanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sieƒá bƒôdzie przetwarza≈Ça obrazy o rozmiarze 224 √ó 224 pikseli, a wiƒôc pierwszym krokiem bƒôdzie\n",
    "przetworzenie. Obiekty Dataset pozwalajƒÖ na wykorzystanie metody map, kt√≥ra przy uczeniu\n",
    "nadzorowanym bƒôdzie otrzymywa≈Ça dwa argumenty (cechy, etykieta) i powinna zwracaƒá je w postaci\n",
    "krotki po przetworzeniu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najprostsza funkcja bƒôdzie po prostu skalowa≈Ça obraz do po≈ºƒÖdanego rozmiaru:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def preprocess(image, label):\n",
    "    resized_image = tf.image.resize(image, [224, 224])\n",
    "    return resized_image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplikujemy jƒÖ do pobranych zbior√≥w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "train_set = train_set_raw.map(preprocess).shuffle(dataset_size).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)\n",
    "test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykorzystujemy tu dodatkowe metody Dataset API tak aby dostarczanie danych nie sta≈Ço siƒô wƒÖskim gard≈Çem procesu uczenia:\n",
    "- shuffle losowo ustawia kolejno≈õƒá pr√≥bek w zbiorze uczƒÖcym,\n",
    "- batch ≈ÇƒÖczy pr√≥bki we wsady o podanej d≈Çugo≈õci (idealnie, powinna to byƒá wielko≈õƒá miniwsadu podczas uczenia),\n",
    "- prefetch zapewnia takie zarzƒÖdzanie buforem, aby zawsze przygotowane by≈Ço ùëõ pr√≥bek gotowych do pobrania (w tym przypadku chcemy, aby podczas przetwarzania miniwsadu przez algorytm uczenia zawsze czeka≈Ç jeden przygotowany kolejny miniwsad).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wy≈õwietlmy pr√≥bkƒô danych po przetworzeniu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 8))\n",
    "# sample_batch = train_set.take(1)\n",
    "# for X_batch, y_batch in sample_batch:\n",
    "#     for index in range(12):\n",
    "#         plt.subplot(3, 4, index + 1)\n",
    "#         plt.imshow(X_batch[index] / 255.0)\n",
    "#         plt.title(\"Class: {}\".format(class_names[y_batch[index]]))\n",
    "#         plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Budowa sieci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaprojektuj prostƒÖ sieƒá konwolucyjnƒÖ, kt√≥ra pozwoli na uzyskanie przyzwoitej dok≈Çadno≈õci klasy- fikacji przetwarzanego zbioru."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pamiƒôtaj o istotnych zasadach:\n",
    "1. W przypadku naszych danych, poniewa≈º sk≈Çadowe RGB pikseli majƒÖ warto≈õci z zakresu 0‚Äì255, musimy pamiƒôtaƒá o normalizacji danych; mo≈ºna u≈ºyƒá do tego warstwy skalujƒÖcej warto≈õci.\n",
    "2. Czƒô≈õƒá wykrywajƒÖca elementy obrazu sk≈Çada siƒô z warstw konwolucyjnych, najczƒô≈õciej przepla- tanych warstwami zbierajƒÖcymi:\n",
    "- g≈Ç√≥wnymi parametrami warstw konwolucyjnych sƒÖ liczba filtr√≥w i rozmiar filtra; za- zwyczaj zaczynamy od wzglƒôdnie niskiej liczby filtr√≥w (np. 32) o wiƒôkszym rozmiarze (np. 7 √ó 7), aby wykryƒá elementarne komponenty obrazu, a na kolejnych warstwach ≈ÇƒÖczymy je w bardziej z≈Ço≈ºone struktury ‚Äì kombinacji jest wiƒôcej, a wiƒôc mamy coraz wiƒôcej filtr√≥w, ale mogƒÖ byƒá mniejszego rozmiaru (np. 3 √ó 3),\n",
    "- zwyczajowo na jednƒÖ warstwƒô konwolucyjnƒÖ przypada≈Ça jedna warstwa zbierajƒÖca (zm- niejszajƒÖca rozmiar ‚Äûobrazu‚Äù), ale czƒôsto stosujemy te≈º kilka (np. 2) warstw kon- wolucyjnych bezpo≈õrednio na sobie.\n",
    "3. Po czƒô≈õci konwolucyjnej typowo nastƒôpuje czƒô≈õƒá gƒôsta, z≈Ço≈ºona z warstw gƒôstych i opcjonalnie regularyzacyjnych (dropout?):\n",
    "- czƒô≈õƒá gƒôsta musi byƒá poprzedzona warstwƒÖ sp≈ÇaszczajƒÖcƒÖ dane, gdy≈º spodziewa siƒô 1- wymiarowej struktury,\n",
    "- ostatnia warstwa musi byƒá dostosowana do charakterystyki zbioru danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Rescaling(scale=1. / 127.5, offset=-1, input_shape=[224, 224, 3]),\n",
    "  tf.keras.layers.Conv2D(32, 7, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(64, 5, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Conv2D(96, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "\n",
    "  tf.keras.layers.Dense(5, activation=\"softmax\")\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "230/230 [==============================] - 202s 874ms/step - loss: 1.5558 - accuracy: 0.3100 - val_loss: 1.3335 - val_accuracy: 0.4392\n",
      "Epoch 2/10\n",
      "230/230 [==============================] - 206s 894ms/step - loss: 1.3701 - accuracy: 0.4168 - val_loss: 1.2793 - val_accuracy: 0.4664\n",
      "Epoch 3/10\n",
      "230/230 [==============================] - 203s 876ms/step - loss: 1.2364 - accuracy: 0.4822 - val_loss: 1.1242 - val_accuracy: 0.5572\n",
      "Epoch 4/10\n",
      "230/230 [==============================] - 215s 931ms/step - loss: 1.0636 - accuracy: 0.5661 - val_loss: 1.0229 - val_accuracy: 0.5935\n",
      "Epoch 5/10\n",
      "230/230 [==============================] - 201s 870ms/step - loss: 0.9290 - accuracy: 0.6337 - val_loss: 1.0289 - val_accuracy: 0.5717\n",
      "Epoch 6/10\n",
      "230/230 [==============================] - 203s 877ms/step - loss: 0.7690 - accuracy: 0.6973 - val_loss: 1.0556 - val_accuracy: 0.5935\n",
      "Epoch 7/10\n",
      "230/230 [==============================] - 201s 869ms/step - loss: 0.6202 - accuracy: 0.7624 - val_loss: 1.2419 - val_accuracy: 0.5808\n",
      "Epoch 8/10\n",
      "230/230 [==============================] - 204s 885ms/step - loss: 0.5126 - accuracy: 0.8023 - val_loss: 1.4070 - val_accuracy: 0.6062\n",
      "Epoch 9/10\n",
      "230/230 [==============================] - 205s 886ms/step - loss: 0.4082 - accuracy: 0.8474 - val_loss: 1.2621 - val_accuracy: 0.5826\n",
      "Epoch 10/10\n",
      "230/230 [==============================] - 210s 910ms/step - loss: 0.4183 - accuracy: 0.8572 - val_loss: 1.3153 - val_accuracy: 0.5463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x231dc72ca30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=[\"sparse_categorical_crossentropy\"], metrics=[\"accuracy\"], optimizer=\"Adam\")\n",
    "model.fit(train_set, validation_data=valid_set, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapisz wynik ewaluacji dla zbioru uczƒÖcego, walidacyjnego i testowego w postaci krotki\n",
    "(acc_train, acc_valid, acc_test) do pikla simple_cnn_acc.pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/230 [==============================] - 50s 213ms/step - loss: 0.3172 - accuracy: 0.9026\n",
      "46/46 [==============================] - 9s 190ms/step - loss: 1.3153 - accuracy: 0.5463\n",
      "31/31 [==============================] - 6s 193ms/step - loss: 1.0391 - accuracy: 0.6104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9026162624359131, 0.5462794899940491, 0.6103542447090149)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "acc = (model.evaluate(train_set)[1], model.evaluate(valid_set)[1], model.evaluate(test_set)[1])\n",
    "\n",
    "with open('simple_cnn_acc.pkl', 'wb') as file:\n",
    "    pickle.dump(acc, file)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie transferowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tym razem wykorzystamy gotowƒÖ, du≈ºo bardziej z≈Ço≈ºonƒÖ sieƒá. Dziƒôki temu, ≈ºe sieƒá bƒôdzie zainicjalizowana wagami, mo≈ºemy znaczƒÖco skr√≥ciƒá czas uczenia.\n",
    "Jako bazowƒÖ wykorzystamy wzglƒôdnie nowoczesnƒÖ sieƒá Xception. Jest ona dostƒôpna w pakiecie\n",
    "tf.keras.applications.xception.\n",
    "Wykorzystamy wcze≈õniej ju≈º za≈Çadowane surowe zbiory danych (..._set_raw)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gotowe modele czƒôsto dostarczajƒÖ w≈Çasnych funkcji przygotowujƒÖcych wej≈õcie w spos√≥b zapewniajƒÖcy optymalne przetwarzanie. Musimy wiƒôc zmieniƒá nieco funkcjƒô przygotowujƒÖcƒÖ dane, dodajƒÖc\n",
    "wywo≈Çanie odpowiedniej metody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    resized_image = tf.image.resize(image, [224, 224])\n",
    "    final_image = tf.keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobaczmy jak tym razem wyglƒÖdajƒÖ wstƒôpnie przetworzone dane; zwr√≥ƒá uwagƒô, ≈ºe poniewa≈º teraz\n",
    "warto≈õci nale≈ºƒÖ ju≈º do zakresu (‚àí1, 1), musimy je odpowiednio przeskalowaƒá (ale w sieci nie\n",
    "bƒôdziemy potrzebowali warstwy skalujƒÖcej):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sample_batch = train_set.take(1)\n",
    "for X_batch, y_batch in sample_batch:\n",
    "    for index in range(12):\n",
    "        plt.subplot(3, 4, index + 1)\n",
    "        plt.imshow(X_batch[index] / 2 + 0.5)\n",
    "        plt.title(\"Class: {}\".format(class_names[y_batch[index]]))\n",
    "        plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Budowa sieci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utw√≥rz model bazowy przy pomocy odpowiedniej metody:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.xception.Xception(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyja≈õnienie:\n",
    "- argument weights zapewnia inicjalizacjƒô wag sieci wynikami uczenia zbiorem ImageNet,\n",
    "- argument include_top sprawi, ≈ºe sieƒá nie bƒôdzie posiada≈Ça g√≥rnych warstw (kt√≥re musimy\n",
    "sami dodaƒá, gdy≈º sƒÖ specyficzne dla danego problemu).\n",
    "\n",
    "Mo≈ºesz wy≈õwietliƒá strukturƒô za≈Çadowanej sieci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_2\n",
      "1 block1_conv1\n",
      "2 block1_conv1_bn\n",
      "3 block1_conv1_act\n",
      "4 block1_conv2\n",
      "5 block1_conv2_bn\n",
      "6 block1_conv2_act\n",
      "7 block2_sepconv1\n",
      "8 block2_sepconv1_bn\n",
      "9 block2_sepconv2_act\n",
      "10 block2_sepconv2\n",
      "11 block2_sepconv2_bn\n",
      "12 conv2d_10\n",
      "13 block2_pool\n",
      "14 batch_normalization_4\n",
      "15 add_12\n",
      "16 block3_sepconv1_act\n",
      "17 block3_sepconv1\n",
      "18 block3_sepconv1_bn\n",
      "19 block3_sepconv2_act\n",
      "20 block3_sepconv2\n",
      "21 block3_sepconv2_bn\n",
      "22 conv2d_11\n",
      "23 block3_pool\n",
      "24 batch_normalization_5\n",
      "25 add_13\n",
      "26 block4_sepconv1_act\n",
      "27 block4_sepconv1\n",
      "28 block4_sepconv1_bn\n",
      "29 block4_sepconv2_act\n",
      "30 block4_sepconv2\n",
      "31 block4_sepconv2_bn\n",
      "32 conv2d_12\n",
      "33 block4_pool\n",
      "34 batch_normalization_6\n",
      "35 add_14\n",
      "36 block5_sepconv1_act\n",
      "37 block5_sepconv1\n",
      "38 block5_sepconv1_bn\n",
      "39 block5_sepconv2_act\n",
      "40 block5_sepconv2\n",
      "41 block5_sepconv2_bn\n",
      "42 block5_sepconv3_act\n",
      "43 block5_sepconv3\n",
      "44 block5_sepconv3_bn\n",
      "45 add_15\n",
      "46 block6_sepconv1_act\n",
      "47 block6_sepconv1\n",
      "48 block6_sepconv1_bn\n",
      "49 block6_sepconv2_act\n",
      "50 block6_sepconv2\n",
      "51 block6_sepconv2_bn\n",
      "52 block6_sepconv3_act\n",
      "53 block6_sepconv3\n",
      "54 block6_sepconv3_bn\n",
      "55 add_16\n",
      "56 block7_sepconv1_act\n",
      "57 block7_sepconv1\n",
      "58 block7_sepconv1_bn\n",
      "59 block7_sepconv2_act\n",
      "60 block7_sepconv2\n",
      "61 block7_sepconv2_bn\n",
      "62 block7_sepconv3_act\n",
      "63 block7_sepconv3\n",
      "64 block7_sepconv3_bn\n",
      "65 add_17\n",
      "66 block8_sepconv1_act\n",
      "67 block8_sepconv1\n",
      "68 block8_sepconv1_bn\n",
      "69 block8_sepconv2_act\n",
      "70 block8_sepconv2\n",
      "71 block8_sepconv2_bn\n",
      "72 block8_sepconv3_act\n",
      "73 block8_sepconv3\n",
      "74 block8_sepconv3_bn\n",
      "75 add_18\n",
      "76 block9_sepconv1_act\n",
      "77 block9_sepconv1\n",
      "78 block9_sepconv1_bn\n",
      "79 block9_sepconv2_act\n",
      "80 block9_sepconv2\n",
      "81 block9_sepconv2_bn\n",
      "82 block9_sepconv3_act\n",
      "83 block9_sepconv3\n",
      "84 block9_sepconv3_bn\n",
      "85 add_19\n",
      "86 block10_sepconv1_act\n",
      "87 block10_sepconv1\n",
      "88 block10_sepconv1_bn\n",
      "89 block10_sepconv2_act\n",
      "90 block10_sepconv2\n",
      "91 block10_sepconv2_bn\n",
      "92 block10_sepconv3_act\n",
      "93 block10_sepconv3\n",
      "94 block10_sepconv3_bn\n",
      "95 add_20\n",
      "96 block11_sepconv1_act\n",
      "97 block11_sepconv1\n",
      "98 block11_sepconv1_bn\n",
      "99 block11_sepconv2_act\n",
      "100 block11_sepconv2\n",
      "101 block11_sepconv2_bn\n",
      "102 block11_sepconv3_act\n",
      "103 block11_sepconv3\n",
      "104 block11_sepconv3_bn\n",
      "105 add_21\n",
      "106 block12_sepconv1_act\n",
      "107 block12_sepconv1\n",
      "108 block12_sepconv1_bn\n",
      "109 block12_sepconv2_act\n",
      "110 block12_sepconv2\n",
      "111 block12_sepconv2_bn\n",
      "112 block12_sepconv3_act\n",
      "113 block12_sepconv3\n",
      "114 block12_sepconv3_bn\n",
      "115 add_22\n",
      "116 block13_sepconv1_act\n",
      "117 block13_sepconv1\n",
      "118 block13_sepconv1_bn\n",
      "119 block13_sepconv2_act\n",
      "120 block13_sepconv2\n",
      "121 block13_sepconv2_bn\n",
      "122 conv2d_13\n",
      "123 block13_pool\n",
      "124 batch_normalization_7\n",
      "125 add_23\n",
      "126 block14_sepconv1\n",
      "127 block14_sepconv1_bn\n",
      "128 block14_sepconv1_act\n",
      "129 block14_sepconv2\n",
      "130 block14_sepconv2_bn\n",
      "131 block14_sepconv2_act\n"
     ]
    }
   ],
   "source": [
    "for index, layer in enumerate(base_model.layers):\n",
    "    print(index, layer.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KorzystajƒÖc z API funkcyjnego Keras dodaj warstwy:\n",
    "- u≈õredniajƒÖcƒÖ warto≈õci wszystkich ‚Äûpikseli‚Äù,\n",
    "- wyj≈õciowƒÖ, gƒôstƒÖ, odpowiedniƒÖ dla problemu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = tf.keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = tf.keras.models.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przeprowad≈∫ uczenie w dw√≥ch krokach:\n",
    "1. Kilka (np. 5) iteracji, podczas kt√≥rych warstwy sieci bazowej bƒôdƒÖ zablokowane; ten krok\n",
    "jest konieczny aby zapobiec ‚Äûzepsuciu‚Äù wag dostarczonych wraz z sieciƒÖ bazowƒÖ ze wzglƒôdu\n",
    "na spodziewane du≈ºe b≈Çƒôdy wynikajƒÖce z braku przyuczenia ‚Äûnowych‚Äù warstw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "230/230 [==============================] - 240s 1s/step - loss: 4.2181 - accuracy: 0.3532 - val_loss: 3.4585 - val_accuracy: 0.3521\n",
      "Epoch 2/5\n",
      "230/230 [==============================] - 240s 1s/step - loss: 2.9127 - accuracy: 0.4539 - val_loss: 3.2938 - val_accuracy: 0.4446\n",
      "Epoch 3/5\n",
      "230/230 [==============================] - 328s 1s/step - loss: 2.5220 - accuracy: 0.5025 - val_loss: 4.9533 - val_accuracy: 0.4229\n",
      "Epoch 4/5\n",
      "230/230 [==============================] - 243s 1s/step - loss: 2.2419 - accuracy: 0.5338 - val_loss: 2.5389 - val_accuracy: 0.4973\n",
      "Epoch 5/5\n",
      "230/230 [==============================] - 258s 1s/step - loss: 2.0156 - accuracy: 0.5683 - val_loss: 2.8403 - val_accuracy: 0.4628\n"
     ]
    }
   ],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "230/230 [==============================] - 246s 1s/step - loss: 1.8755 - accuracy: 0.5927 - val_loss: 3.4620 - val_accuracy: 0.4882\n",
      "Epoch 2/10\n",
      "230/230 [==============================] - 248s 1s/step - loss: 1.7297 - accuracy: 0.6039 - val_loss: 2.2371 - val_accuracy: 0.4955\n",
      "Epoch 3/10\n",
      "230/230 [==============================] - 251s 1s/step - loss: 1.4579 - accuracy: 0.6479 - val_loss: 2.7337 - val_accuracy: 0.4755\n",
      "Epoch 4/10\n",
      "212/230 [==========================>...] - ETA: 24s - loss: 1.5082 - accuracy: 0.6360"
     ]
    }
   ],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "model.fit(train_set, validation_data=valid_set,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapisz wynik ewaluacji dla zbioru uczƒÖcego, walidacyjnego i testowego w postaci krotki\n",
    "(acc_train, acc_valid, acc_test) do pikla xception_acc.pkl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (model.evaluate(train_set)[1], model.evaluate(valid_set)[1], model.evaluate(test_set)[1])\n",
    "\n",
    "with open('xception_acc.pkl', 'wb') as file:\n",
    "    pickle.dump(acc, file)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}